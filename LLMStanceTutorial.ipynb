{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM for stance detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will use the Large Language Models to perform stance detection. The task is to classify the stance of a given text towards a given target. And as I mentioned, the LLMs can be used to perform this task with Few-Shot or even Zero-Shot learning. I will provide a Zero-Shot learning example in this notebook.\n",
    "\n",
    "I will demonstrate how to use the LLMs for stance detection using the `transformers` library. We will use the `google/gemma-3-12b-it` model for this task. The `transformers` library provides a simple API to use the LLMs for various NLP tasks.\n",
    "\n",
    "We will use the `AutoModelForCausalLM` class to load the model and the `AutoTokenizer` class to load the tokenizer. Causal language models are the models that can generate the next token given the previous tokens and tokenizer is used to convert the text into tokens that can be fed into the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Mar 22 20:28:08 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 560.35.05              Driver Version: 560.35.05      CUDA Version: 12.6     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA A40                     On  |   00000000:1D:00.0 Off |                    0 |\n",
      "|  0%   26C    P8             20W /  300W |       1MiB /  46068MiB |      0%   E. Process |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126 \n",
    "!pip3 install -U transformers\n",
    "!pip3 install -U accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Remove #SemST from the tweet\n",
    "\n",
    "stance = pd.read_csv(\"../../data/SemEval2016-testdata-taskA-all-annotations.csv\")\n",
    "stance[\"Tweet\"] = stance[\"Tweet\"].str.replace(\"#SemST\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model and tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Huggingface Transformers library\n",
    "# https://huggingface.co/transformers/\n",
    "# clear jupyter notebook output\n",
    "from IPython.display import clear_output\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "model_name = \"google/gemma-3-12b-it\"  # LLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)  # Load tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device)  # Load model\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specify the prompt template\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start format the prompt template, we need to understand what the inputs should look like and why we need to format the prompt template like this.\n",
    "\n",
    "For training a Large Language Model (with 'trained' referring specifically to the model after human alignment) we need to provide the model with a prompt that contains the input text and the target text. And the model should be able to distinguish between human input and desired output. Therefore, we will roughly see two types of prompt templates: one will only distinguish between the human input and model output and the other will also provide the instruction. The only difference between the two is that the former will treat the instruction as a part of the input text and the latter will treat the instruction as a separate entity.\n",
    "\n",
    "Based on the [technical report](https://storage.googleapis.com/deepmind-media/gemma/Gemma3Report.pdf) provided by Google, they used the first type of prompt template. Therefore, we will also follow the same format for the prompt template.\n",
    "\n",
    "The prompt template should look like this:\n",
    "\n",
    "```\n",
    "[BOS]<start_of_turn>user\n",
    "Who are you?<end_of_turn>\n",
    "<start_of_turn>model\n",
    "My name is Gemma!<end_of_turn>\n",
    "<start_of_turn>user\n",
    "What is 2+2?<end_of_turn>\n",
    "<start_of_turn>model\n",
    "```\n",
    "\n",
    "Where `<start_of_turn>` and `<end_of_turn>` are the special tokens that are used to indicate the start and end of the sequence. `user` and `model` are the special tokens that are used to indicate whether the content is from user or LLM. \n",
    "\n",
    "Fortunately, for inference-only tasks (such as zero-shot or few-shot learning), we don’t need to provide the model with the target text—only the input text is required. By using the `transformers` library’s `pipeline` function, we can format the input in a way that is applied uniformly across different LLMs, even if they use different chat templates.\n",
    "\n",
    "To reuse the prompt template for different inputs, we will create a `f` string that will take the input text and the target text as input and return the formatted prompt template. If you are not familiar with the `f` string, you can think of it as a string that can take variables as input and return the formatted string. More details can be found [here](https://realpython.com/python-f-strings/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take an example to understand how the prompt template will look like for the stance detection task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = stance.loc[0, \"Target\"]\n",
    "tweet = stance.loc[0, \"Tweet\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: Atheism\n",
      "Tweet: dear lord thank u for all of ur blessings forgive my sins lord give me strength and energy for this busy day ahead #blessed #hope \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Topic: {topic}\\nTweet: {tweet}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the prompt template\n",
    "prompt = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": [{\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}]\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": f\"\"\"\n",
    "            Instruction: You have assumed the role of a human annotator. In this task, you will be presented with a tweet, delimited by triple backticks, concerning the {topic}. Please make the following assessment:\n",
    "            (1) Determine whether the tweet discusses the topic of the {topic}. If so, please indicate whether the Twitter user who posted the tweet favors, opposes, or has no opinion on the {topic}.\n",
    "                                         \n",
    "            Your response should be formatted as follows: \"Stance: [FAVOR, AGAINST, NONE]\"\n",
    "\n",
    "            Tweet: ```{tweet}```\"\"\"}\n",
    "        ]\n",
    "    }\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"Prompt: [{'role': 'system', 'content': [{'type': 'text', 'text': 'You are a \"\n",
      " \"helpful assistant.'}]}, {'role': 'user', 'content': [{'type': 'text', \"\n",
      " \"'text': '\\\\n            Instruction: You have assumed the role of a human \"\n",
      " 'annotator. In this task, you will be presented with a tweet, delimited by '\n",
      " 'triple backticks, concerning the Atheism. Please make the following '\n",
      " 'assessment:\\\\n            (1) Determine whether the tweet discusses the '\n",
      " 'topic of the Atheism. If so, please indicate whether the Twitter user who '\n",
      " 'posted the tweet favors, opposes, or has no opinion on the '\n",
      " 'Atheism.\\\\n                                         \\\\n            Your '\n",
      " 'response should be formatted as follows: \"Stance: [FAVOR, AGAINST, '\n",
      " 'NONE]\"\\\\n\\\\n            Tweet: ```dear lord thank u for all of ur blessings '\n",
      " 'forgive my sins lord give me strength and energy for this busy day ahead '\n",
      " \"#blessed #hope ```'}]}]\\n\")\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(f\"Prompt: {prompt}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM inference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the input text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we already have the raw input text, we will need to transform it into the format that the model can understand. We will use the `AutoTokenizer` class to convert the input text into tokens that can be fed into the model. The `AutoTokenizer` class will automatically select the appropriate tokenizer for the model.\n",
    "\n",
    "For more information on understand how tokenizer works and how to use the `AutoTokenizer` class, you can refer to the [official documentation](https://huggingface.co/transformers/model_doc/auto.html#autotokenizer).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: tensor([[     2,    105,   2364,    107,   3048,    659,    496,  11045,  16326,\n",
      "         236761,    108,  74279, 236787,   1599,    735,  12718,    506,   3853,\n",
      "            529,    496,   3246,  24740,   1277, 236761,    799,    672,   4209,\n",
      "         236764,    611,    795,    577,   6212,    607,    496,  21866, 236764,\n",
      "         215131,    684,  22178,   1063,  81982, 236764,  13899,    506, 234681,\n",
      "           1929, 236761,   7323,   1386,    506,   2269,  10834, 236787,    107,\n",
      "            148, 236769, 236770, 236768,  32814,   3363,    506,  21866,  39817,\n",
      "            506,  10562,    529,    506, 234681,   1929, 236761,   1637,    834,\n",
      "         236764,   5091,  10128,   3363,    506,   9526,   2430,   1015,  12551,\n",
      "            506,  21866,  69656, 236764, 122139, 236764,    653,    815,    951,\n",
      "           8737,    580,    506, 234681,   1929, 236761,    107,    167,    146,\n",
      "            107,    148,  11069,   3072,   1374,    577,  45047,    618,   5238,\n",
      "         236787,    623,    894,    831, 236787,    870,   4815, 198502, 236764,\n",
      "         183066, 236764,  96869,  50190,    108,    148,  75427, 236787,  31608,\n",
      "         167521,  29398,   7806,    559,    573,    784,    529,   5425,  56274,\n",
      "          49233,   1041,  44769,  29398,   2583,    786,   6332,    532,   2778,\n",
      "            573,    672,  13181,   1719,   7531,    997,   1782,  26236,    997,\n",
      "          61475,  31608,    106,    107,    105,   4368,    107]],\n",
      "       device='cuda:0')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the prompt\n",
    "inputs = tokenizer.apply_chat_template(prompt, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(device)\n",
    "# Take a look at the tokens\n",
    "print(f\"Tokens: {inputs}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant.\n",
      "\n",
      "Instruction: You have assumed the role of a human annotator. In this task, you will be presented with a tweet, delimited by triple backticks, concerning the Atheism. Please make the following assessment:\n",
      "            (1) Determine whether the tweet discusses the topic of the Atheism. If so, please indicate whether the Twitter user who posted the tweet favors, opposes, or has no opinion on the Atheism.\n",
      "                                         \n",
      "            Your response should be formatted as follows: \"Stance: [FAVOR, AGAINST, NONE]\"\n",
      "\n",
      "            Tweet: ```dear lord thank u for all of ur blessings forgive my sins lord give me strength and energy for this busy day ahead #blessed #hope ```<end_of_turn>\n",
      "<start_of_turn>model\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: the tokenized prompt can always be decoded back to the original prompt\n",
    "# Decode the tokenized prompt\n",
    "decoded_prompt = tokenizer.decode(inputs[0])\n",
    "print(decoded_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed the input text (tokens) into the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output: Stance: AGAINST\n",
      "Human annotation: AGAINST\n"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(inputs, max_new_tokens=20)  # Generate the model output\n",
    "# Decode the generated output\n",
    "generated_output = tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True)\n",
    "# Print the generated output and compare with human annotation\n",
    "print(\n",
    "    f\"Generated Output: {generated_output}\\nHuman annotation: {stance.loc[0,'Stance']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several parameters that we can use to control the output of the model. We used the `max_length` parameter to control the maximum length of the output. We alsod use the `return_tensors` parameter to control the output format. We set it to `pt` to get the output in PyTorch tensors format.\n",
    "\n",
    "Besides these parameters, we can also use the `temperature` parameter to control the randomness of the output. We can use the `top_k` and `top_p` parameters to control the diversity of the output. We can also use the `num_return_sequences` parameter to control the number of output sequences.\n",
    "\n",
    "There is a great explanation of temperature parameter (which is also the parameter you will use for OpenAI's Models) in the [blog](https://lukesalamone.github.io/posts/what-is-temperature/)\n",
    "\n",
    "Feel free to play around with these parameters to see how they affect the output of the model.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
